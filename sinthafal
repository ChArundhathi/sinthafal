# Import libraries
import os
import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
import chromadb
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.schema import Document

# Step 1: Crawl and Scrape Website
def scrape_website(url):
    """
    Function to scrape website content.
    """
    try:
        response = requests.get(url)
        if response.status_code != 200:
            print(f"Failed to fetch {url}")
            return ""
        soup = BeautifulSoup(response.text, "html.parser")
        texts = soup.stripped_strings  # Extract readable text
        return "\n".join(texts)
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return ""

# List of websites to scrape
urls = [
    "https://www.uchicago.edu/",
    "https://www.washington.edu/",
    "https://www.stanford.edu/",
    "https://und.edu/"
]

# Scrape content from websites
website_texts = ""
for url in urls:
    print(f"Scraping {url}...")
    website_texts += scrape_website(url)

print("Scraping completed!")

# Step 2: Chunk the Data
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
chunks = text_splitter.split_text(website_texts)
print(f"Number of chunks created: {len(chunks)}")

# Step 3: Generate Embeddings Using Hugging Face
print("Generating embeddings...")

# Load Hugging Face model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Convert chunks into embeddings
embeddings = embedding_model.encode(chunks, show_progress_bar=True)
